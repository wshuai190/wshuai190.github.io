---
title: "2D Matryoshka Training for Information Retrieval"
collection: publications
permalink: /publication/2025-05-14-2dmse-ir
excerpt: 
date: 2025-02-10
page_type: "Reproduce"
venue: 'Accepted SIGIR-2025'
paperurl: 'https://arxiv.org/abs/2411.17299'
citation: 'Shuai Wang, Shengyao Zhuang, Bevan Koopman and Guido Zuccon. 2025. 2D Matryoshka Training for Information Retrieval. (Accepted SIGIR-2025).'
---
## Abstract
2D Matryoshka Training is an advanced embedding representation training approach designed to train an encoder model simultaneously across various layer-dimension setups. This method has demonstrated higher effectiveness in Semantic Text Similarity (STS) tasks over traditional training approaches when using sub-layers for embeddings. Despite its success, discrepancies exist between two published implementations, leading to varied comparative results with baseline models. In this reproducibility study, we implement and evaluate both versions of 2D Matryoshka Training on STS tasks and extend our analysis to retrieval tasks. Our findings indicate that while both versions achieve higher effectiveness than traditional Matryoshka training on sub-dimensions, and traditional full-sized model training approaches, they do not outperform models trained separately on specific sub-layer and sub-dimension setups. Moreover, these results generalize well to retrieval tasks, both in supervised (MSMARCO) and zero-shot (BEIR) settings. Further explorations of different loss computations reveals more suitable implementations for retrieval tasks, such as incorporating full-dimension loss and training on a broader range of target dimensions. Conversely, some intuitive approaches, such as fixing document encoders to full model outputs, do not yield improvements. Our reproduction code is available at https://github.com/ielab/2DMSE-Reproduce